{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Umpc_Scot9d8"
      },
      "source": [
        "#MOUNT GOOGLE DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVlFSLmsSd7c",
        "outputId": "be57b15d-8d7c-421c-a10f-267b74e91a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDLz9rmauB_J"
      },
      "source": [
        "#INSTALL DEPENDENCIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6O0ocfCDTkmm"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q55GPQVFeU7d",
        "outputId": "07642737-43b0-4bb7-95e7-5cd6d553ec59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.2.2)\n",
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (5.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.21.6)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis) (3.7)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis) (7.1.2)\n",
            "Installing collected packages: ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.5 ekphrasis-0.5.4 ftfy-6.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEnKfM_r-99T",
        "outputId": "b160e311-e7d3-479e-ddc0-9633289c37b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textdistance\n",
            "  Downloading textdistance-4.2.2-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: textdistance\n",
            "Successfully installed textdistance-4.2.2\n"
          ]
        }
      ],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTnIrNOTuLGz"
      },
      "source": [
        "#UTILITIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARNJXLBXSmgv"
      },
      "outputs": [],
      "source": [
        "path='/content/gdrive/My Drive/URLBUG/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L2LXNvEeOJB",
        "outputId": "bae1607f-f681-4b69-f51c-24b964bc1005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ],
      "source": [
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import re\n",
        "seg_eng = Segmenter(corpus=\"twitter\") \n",
        "_digits = re.compile('\\d')\n",
        "def contains_digits(d):\n",
        "     return bool(_digits.search(d))\n",
        "def converter(listoflist):\n",
        "    lst=[]\n",
        "   \n",
        "    for ls in listoflist:\n",
        "        \n",
        "        if(type(ls) is list):\n",
        "            for e in ls:\n",
        "                if(e !=''):\n",
        "                    lst.append((e))\n",
        "        else:\n",
        "           lst.append(ls)\n",
        "    return lst  \n",
        "def tokenparts(words):\n",
        "    finalwords=[]\n",
        "    for w in words:\n",
        "       w=re.sub('\\W+',' ',w)\n",
        "       w=re.sub('\\d+',' ',w)\n",
        "       w=re.sub('_',' ',w)\n",
        "       w=re.sub(';',' ',w)\n",
        "       \n",
        "       if(w!='' and w!='None' ):\n",
        "#           words.append(w.split('_'))\n",
        "#           words.remove(w)\n",
        "          try:\n",
        "             subwords=(seg_eng.segment(w))\n",
        "             finalwords.append(subwords.split(' '))\n",
        "          except:\n",
        "            finalwords.append(w.split(' '))\n",
        "    return (finalwords)\n",
        "def obtainwordlist(url):\n",
        "    words=re.findall(r'\\w+\\b', url)\n",
        "    \n",
        "    finalwords=tokenparts(words)\n",
        "    final=converter(finalwords)\n",
        "    \n",
        "    return final "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmLKylo3eEHv"
      },
      "outputs": [],
      "source": [
        "def converter(listoflist):\n",
        "    lst=[]\n",
        "   \n",
        "    for ls in listoflist:\n",
        "        \n",
        "        if(type(ls) is list):\n",
        "            for e in ls:\n",
        "                if(e !=''):\n",
        "                    lst.append((e))\n",
        "        else:\n",
        "           lst.append(ls)\n",
        "    return lst  \n",
        "def tokenparts(words):\n",
        "    finalwords=[]\n",
        "    for w in words:\n",
        "       w=re.sub('\\W+',' ',w)\n",
        "       w=re.sub('\\d+',' ',w)\n",
        "       w=re.sub('_',' ',w)\n",
        "       w=re.sub(';',' ',w)\n",
        "       \n",
        "       if(w!='' and w!='None' ):\n",
        "#           words.append(w.split('_'))\n",
        "#           words.remove(w)\n",
        "          try:\n",
        "             subwords=(seg_eng.segment(w))\n",
        "             finalwords.append(subwords.split(' '))\n",
        "          except:\n",
        "            finalwords.append(w.split(' '))\n",
        "    return (finalwords)\n",
        "def obtainwordlist(url):\n",
        "    words=re.findall(r'\\w+\\b', url)\n",
        "    \n",
        "    finalwords=tokenparts(words)\n",
        "    final=converter(finalwords)\n",
        "    \n",
        "    return final "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJVcmwxgd9i7"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zy8EvDiGkns"
      },
      "outputs": [],
      "source": [
        "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
        "char_dict = {}\n",
        "for i, char in enumerate(alphabet):\n",
        "    char_dict[char] = i + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Up3ny-XuOi-"
      },
      "source": [
        "#LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL9P7MKmHWfN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import codecs,re\n",
        "codecs.register_error(\"strict\", codecs.ignore_errors)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "def trainingData():\n",
        "\n",
        "    data_dir_path = path\n",
        "    Legitimate = (pd.read_csv(data_dir_path+'Legitimate/Leg_Training.csv',encoding=\"utf-8\") )\n",
        "    Malicious = pd.read_csv(data_dir_path+'Phishing/Phish_Training.csv',encoding=\"utf-8\") \n",
        "    mal=len(Malicious)\n",
        "    Leg=Legitimate.loc[0:(len(Malicious)-1),:]   \n",
        "    frame1=([Leg,Malicious])\n",
        "    url=np.concatenate(frame1).tolist()\n",
        "    leg=len(Leg)\n",
        "    \n",
        "    print(leg)\n",
        "    print(mal)\n",
        "    label0=np.zeros((leg)).astype(int)\n",
        "    label1=np.ones((mal)).astype(int)\n",
        "    label=(np.concatenate((label0,label1)))\n",
        "    label=pd.Series(label.tolist())\n",
        "    d={'url':url,'label':label}\n",
        "    url_data=pd.DataFrame(d, columns=['url','label'])\n",
        "    return url_data['url'],url_data['label']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOnpXSM0Wahd"
      },
      "outputs": [],
      "source": [
        "def process(urls):\n",
        "  urllist=[]\n",
        "  for url in urls:\n",
        "  \n",
        "    urllist.append(url[0])\n",
        "  return urllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPlcYgpXfnlO"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "ord_enc = OrdinalEncoder()\n",
        "def train_BLE():\n",
        "       filename=path+'/Ensemble/BLE/TrainFeatures_multiprocess1.csv'\n",
        "       Featureset = pd.read_csv(filename,encoding=\"utf-8\") \n",
        "       Featureset=Featureset.fillna(-1)\n",
        "       Featureset=Featureset.dropna()\n",
        "       Featureset[\"country\"] = ord_enc.fit_transform(Featureset[[\"country\"]])\n",
        "       labels=Featureset['label']\n",
        "       urls=Featureset['url']\n",
        "       print(np.shape(Featureset))\n",
        "       Features=Featureset.drop(['url','label'], axis=1)\n",
        "       print(np.shape(Features))\n",
        "       Features=Features.replace('?',-1)\n",
        "       Features=Features.replace('TRUE',1)\n",
        "       Features=Features.replace('True',1)\n",
        "       Features=Features.replace('true',1)\n",
        "       Features=Features.replace('FALSE',0)\n",
        "       Features=Features.replace('False',0)\n",
        "       Features=Features.replace('False',0)\n",
        "       Features=Features.replace('0',0)\n",
        "       Features.fillna(0)\n",
        "       return Features,urls,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAV9kHshFnhQ",
        "outputId": "8f69fac0-ee5b-416d-a50b-cc7e9e89a676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96693\n",
            "96693\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, Embedding, Dense, Flatten, Bidirectional\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "X,y=trainingData()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=process(X)"
      ],
      "metadata": {
        "id": "drZRVrNjvfxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall_keras = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall_keras\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision_keras = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision_keras\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    return tn / (tn + fp + K.epsilon())\n",
        "\n",
        "\n",
        "def negative_predictive_value(y_true, y_pred):\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "    return tn / (tn + fn + K.epsilon())\n",
        "\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    p = precision(y_true, y_pred)\n",
        "    r = recall(y_true, y_pred)\n",
        "    return 2 * ((p * r) / (p + r + K.epsilon()))\n",
        "\n",
        "\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "    y_pred = K.clip(y_pred, 0, 1)\n",
        "\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    num = (1 + beta ** 2) * (p * r)\n",
        "    den = (beta ** 2 * p + r + K.epsilon())\n",
        "    return K.mean(num / den)\n",
        "\n",
        "\n",
        "def matthews_correlation_coefficient(y_true, y_pred):\n",
        "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
        "    fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n",
        "    fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n",
        "\n",
        "    num = tp * tn - fp * fn\n",
        "    den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n",
        "    return num / K.sqrt(den + K.epsilon())\n",
        "\n",
        "\n",
        "def equal_error_rate(y_true, y_pred):\n",
        "    n_imp = tf.count_nonzero(tf.equal(y_true, 0), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "    n_gen = tf.count_nonzero(tf.equal(y_true, 1), dtype=tf.float32) + tf.constant(K.epsilon())\n",
        "\n",
        "    scores_imp = tf.boolean_mask(y_pred, tf.equal(y_true, 0))\n",
        "    scores_gen = tf.boolean_mask(y_pred, tf.equal(y_true, 1))\n",
        "\n",
        "    loop_vars = (tf.constant(0.0), tf.constant(1.0), tf.constant(0.0))\n",
        "    cond = lambda t, fpr, fnr: tf.greater_equal(fpr, fnr)\n",
        "    body = lambda t, fpr, fnr: (\n",
        "        t + 0.001,\n",
        "        tf.divide(tf.count_nonzero(tf.greater_equal(scores_imp, t), dtype=tf.float32), n_imp),\n",
        "        tf.divide(tf.count_nonzero(tf.less(scores_gen, t), dtype=tf.float32), n_gen)\n",
        "    )\n",
        "    t, fpr, fnr = tf.while_loop(cond, body, loop_vars, back_prop=False)\n",
        "    eer = (fpr + fnr) / 2\n",
        "\n",
        "    return eer"
      ],
      "metadata": {
        "id": "dK-a8-b-vufJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDv6eKWYWvqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd25754f-221d-457a-861c-d9601f42500f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 295 unique tokens.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y,random_state=42)\n",
        "# Preprocess data for training.\n",
        "max_chars = 20000\n",
        "maxlen = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_chars, char_level=True)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "import pickle\n",
        "file_name = path+\"/Models/tokenizer_lstm.pkl\"\n",
        "# save\n",
        "pickle.dump(tokenizer, open(file_name, \"wb\"))\n",
        "\n",
        "Trainsequences = tokenizer.texts_to_sequences(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "trainingdata = pad_sequences(Trainsequences, maxlen=maxlen)\n",
        "traininglabels=y_train\n",
        "Testsequences = tokenizer.texts_to_sequences(X_test)\n",
        "testingdata=pad_sequences(Testsequences, maxlen=maxlen)\n",
        "testinglabels=y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PTbnT8MFovp"
      },
      "source": [
        "#MODELLING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gl3TyhNIuOH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Activation,SpatialDropout1D,Bidirectional\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import regularizers\n",
        "# Define callbacks for Keras.\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_matthews_correlation_coefficient', \n",
        "    patience=10, \n",
        "    min_delta=0.001, \n",
        "    mode='max')\n",
        "\n",
        "def lstm(tokenizer):\n",
        "      num_chars = len(tokenizer.word_index)+1\n",
        "      embedding_vector_length = 128\n",
        "\n",
        "      # Create model for training.\n",
        "      model = Sequential()\n",
        "      model.add(Embedding(num_chars, embedding_vector_length, input_length=maxlen))\n",
        "      model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
        "      model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
        "      model.add(Dense(1, activation='sigmoid'))\n",
        "      model.summary()\n",
        "      model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[matthews_correlation_coefficient])\n",
        "      return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_early_stopping = EarlyStopping(\n",
        "    monitor='val_matthews_correlation_coefficient', \n",
        "    patience=10, \n",
        "    min_delta=0.001, \n",
        "    mode='max')"
      ],
      "metadata": {
        "id": "_KhaJWplzn4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTH2Dy5IGblL"
      },
      "source": [
        "##TRAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze6o7TiVGcy0",
        "outputId": "d9bc0d7f-8292-4b11-fa59-0f756d94b991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 128, 128)          37888     \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirecti  (None, 128, 256)         263168    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirecti  (None, 128)              164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 465,537\n",
            "Trainable params: 465,537\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "102/102 [==============================] - 252s 2s/step - loss: 0.3513 - matthews_correlation_coefficient: 0.6787 - val_loss: 0.2433 - val_matthews_correlation_coefficient: 0.7996\n",
            "Epoch 2/50\n",
            "102/102 [==============================] - 238s 2s/step - loss: 0.2217 - matthews_correlation_coefficient: 0.8193 - val_loss: 0.1959 - val_matthews_correlation_coefficient: 0.8456\n",
            "Epoch 3/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.1913 - matthews_correlation_coefficient: 0.8485 - val_loss: 0.1871 - val_matthews_correlation_coefficient: 0.8546\n",
            "Epoch 4/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.1723 - matthews_correlation_coefficient: 0.8656 - val_loss: 0.1597 - val_matthews_correlation_coefficient: 0.8743\n",
            "Epoch 5/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.1584 - matthews_correlation_coefficient: 0.8782 - val_loss: 0.1466 - val_matthews_correlation_coefficient: 0.8872\n",
            "Epoch 6/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.1481 - matthews_correlation_coefficient: 0.8861 - val_loss: 0.1406 - val_matthews_correlation_coefficient: 0.8940\n",
            "Epoch 7/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.1401 - matthews_correlation_coefficient: 0.8953 - val_loss: 0.1256 - val_matthews_correlation_coefficient: 0.9050\n",
            "Epoch 8/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.1296 - matthews_correlation_coefficient: 0.9020 - val_loss: 0.1183 - val_matthews_correlation_coefficient: 0.9106\n",
            "Epoch 9/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.1226 - matthews_correlation_coefficient: 0.9076 - val_loss: 0.1127 - val_matthews_correlation_coefficient: 0.9146\n",
            "Epoch 10/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.1161 - matthews_correlation_coefficient: 0.9125 - val_loss: 0.1073 - val_matthews_correlation_coefficient: 0.9198\n",
            "Epoch 11/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.1102 - matthews_correlation_coefficient: 0.9171 - val_loss: 0.1017 - val_matthews_correlation_coefficient: 0.9241\n",
            "Epoch 12/50\n",
            "102/102 [==============================] - 232s 2s/step - loss: 0.1055 - matthews_correlation_coefficient: 0.9218 - val_loss: 0.0992 - val_matthews_correlation_coefficient: 0.9247\n",
            "Epoch 13/50\n",
            "102/102 [==============================] - 233s 2s/step - loss: 0.1020 - matthews_correlation_coefficient: 0.9231 - val_loss: 0.0988 - val_matthews_correlation_coefficient: 0.9281\n",
            "Epoch 14/50\n",
            "102/102 [==============================] - 232s 2s/step - loss: 0.1003 - matthews_correlation_coefficient: 0.9252 - val_loss: 0.0935 - val_matthews_correlation_coefficient: 0.9311\n",
            "Epoch 15/50\n",
            "102/102 [==============================] - 231s 2s/step - loss: 0.0945 - matthews_correlation_coefficient: 0.9289 - val_loss: 0.0953 - val_matthews_correlation_coefficient: 0.9288\n",
            "Epoch 16/50\n",
            "102/102 [==============================] - 230s 2s/step - loss: 0.0917 - matthews_correlation_coefficient: 0.9314 - val_loss: 0.0899 - val_matthews_correlation_coefficient: 0.9343\n",
            "Epoch 17/50\n",
            "102/102 [==============================] - 231s 2s/step - loss: 0.0903 - matthews_correlation_coefficient: 0.9324 - val_loss: 0.0898 - val_matthews_correlation_coefficient: 0.9354\n",
            "Epoch 18/50\n",
            "102/102 [==============================] - 230s 2s/step - loss: 0.0867 - matthews_correlation_coefficient: 0.9350 - val_loss: 0.0856 - val_matthews_correlation_coefficient: 0.9383\n",
            "Epoch 19/50\n",
            "102/102 [==============================] - 229s 2s/step - loss: 0.0832 - matthews_correlation_coefficient: 0.9376 - val_loss: 0.0854 - val_matthews_correlation_coefficient: 0.9383\n",
            "Epoch 20/50\n",
            "102/102 [==============================] - 229s 2s/step - loss: 0.0796 - matthews_correlation_coefficient: 0.9411 - val_loss: 0.0828 - val_matthews_correlation_coefficient: 0.9401\n",
            "Epoch 21/50\n",
            "102/102 [==============================] - 228s 2s/step - loss: 0.0759 - matthews_correlation_coefficient: 0.9431 - val_loss: 0.0860 - val_matthews_correlation_coefficient: 0.9356\n",
            "Epoch 22/50\n",
            "102/102 [==============================] - 229s 2s/step - loss: 0.0739 - matthews_correlation_coefficient: 0.9450 - val_loss: 0.0807 - val_matthews_correlation_coefficient: 0.9426\n",
            "Epoch 23/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.0722 - matthews_correlation_coefficient: 0.9457 - val_loss: 0.0817 - val_matthews_correlation_coefficient: 0.9424\n",
            "Epoch 24/50\n",
            "102/102 [==============================] - 233s 2s/step - loss: 0.0689 - matthews_correlation_coefficient: 0.9489 - val_loss: 0.0830 - val_matthews_correlation_coefficient: 0.9419\n",
            "Epoch 25/50\n",
            "102/102 [==============================] - 232s 2s/step - loss: 0.0666 - matthews_correlation_coefficient: 0.9511 - val_loss: 0.0773 - val_matthews_correlation_coefficient: 0.9453\n",
            "Epoch 26/50\n",
            "102/102 [==============================] - 231s 2s/step - loss: 0.0663 - matthews_correlation_coefficient: 0.9511 - val_loss: 0.0759 - val_matthews_correlation_coefficient: 0.9449\n",
            "Epoch 27/50\n",
            "102/102 [==============================] - 230s 2s/step - loss: 0.0616 - matthews_correlation_coefficient: 0.9545 - val_loss: 0.0828 - val_matthews_correlation_coefficient: 0.9431\n",
            "Epoch 28/50\n",
            "102/102 [==============================] - 230s 2s/step - loss: 0.0599 - matthews_correlation_coefficient: 0.9556 - val_loss: 0.0823 - val_matthews_correlation_coefficient: 0.9405\n",
            "Epoch 29/50\n",
            "102/102 [==============================] - 229s 2s/step - loss: 0.0600 - matthews_correlation_coefficient: 0.9554 - val_loss: 0.0832 - val_matthews_correlation_coefficient: 0.9434\n",
            "Epoch 30/50\n",
            "102/102 [==============================] - 229s 2s/step - loss: 0.0561 - matthews_correlation_coefficient: 0.9592 - val_loss: 0.0815 - val_matthews_correlation_coefficient: 0.9458\n",
            "Epoch 31/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.0552 - matthews_correlation_coefficient: 0.9595 - val_loss: 0.0801 - val_matthews_correlation_coefficient: 0.9460\n",
            "Epoch 32/50\n",
            "102/102 [==============================] - 240s 2s/step - loss: 0.0523 - matthews_correlation_coefficient: 0.9619 - val_loss: 0.0769 - val_matthews_correlation_coefficient: 0.9485\n",
            "Epoch 33/50\n",
            "102/102 [==============================] - 238s 2s/step - loss: 0.0496 - matthews_correlation_coefficient: 0.9644 - val_loss: 0.0738 - val_matthews_correlation_coefficient: 0.9497\n",
            "Epoch 34/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.0486 - matthews_correlation_coefficient: 0.9649 - val_loss: 0.0740 - val_matthews_correlation_coefficient: 0.9493\n",
            "Epoch 35/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0468 - matthews_correlation_coefficient: 0.9658 - val_loss: 0.0750 - val_matthews_correlation_coefficient: 0.9505\n",
            "Epoch 36/50\n",
            "102/102 [==============================] - 237s 2s/step - loss: 0.0457 - matthews_correlation_coefficient: 0.9665 - val_loss: 0.0790 - val_matthews_correlation_coefficient: 0.9481\n",
            "Epoch 37/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0423 - matthews_correlation_coefficient: 0.9694 - val_loss: 0.0794 - val_matthews_correlation_coefficient: 0.9482\n",
            "Epoch 38/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.0474 - matthews_correlation_coefficient: 0.9657 - val_loss: 0.0798 - val_matthews_correlation_coefficient: 0.9481\n",
            "Epoch 39/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0421 - matthews_correlation_coefficient: 0.9696 - val_loss: 0.0798 - val_matthews_correlation_coefficient: 0.9460\n",
            "Epoch 40/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0388 - matthews_correlation_coefficient: 0.9718 - val_loss: 0.0769 - val_matthews_correlation_coefficient: 0.9514\n",
            "Epoch 41/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0376 - matthews_correlation_coefficient: 0.9732 - val_loss: 0.0772 - val_matthews_correlation_coefficient: 0.9517\n",
            "Epoch 42/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.0338 - matthews_correlation_coefficient: 0.9758 - val_loss: 0.0804 - val_matthews_correlation_coefficient: 0.9514\n",
            "Epoch 43/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.0342 - matthews_correlation_coefficient: 0.9757 - val_loss: 0.0767 - val_matthews_correlation_coefficient: 0.9531\n",
            "Epoch 44/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0328 - matthews_correlation_coefficient: 0.9766 - val_loss: 0.0807 - val_matthews_correlation_coefficient: 0.9519\n",
            "Epoch 45/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0308 - matthews_correlation_coefficient: 0.9783 - val_loss: 0.0779 - val_matthews_correlation_coefficient: 0.9531\n",
            "Epoch 46/50\n",
            "102/102 [==============================] - 243s 2s/step - loss: 0.0301 - matthews_correlation_coefficient: 0.9788 - val_loss: 0.0867 - val_matthews_correlation_coefficient: 0.9518\n",
            "Epoch 47/50\n",
            "102/102 [==============================] - 241s 2s/step - loss: 0.0267 - matthews_correlation_coefficient: 0.9812 - val_loss: 0.0839 - val_matthews_correlation_coefficient: 0.9522\n",
            "Epoch 48/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0265 - matthews_correlation_coefficient: 0.9809 - val_loss: 0.0839 - val_matthews_correlation_coefficient: 0.9515\n",
            "Epoch 49/50\n",
            "102/102 [==============================] - 234s 2s/step - loss: 0.0271 - matthews_correlation_coefficient: 0.9811 - val_loss: 0.0876 - val_matthews_correlation_coefficient: 0.9524\n",
            "Epoch 50/50\n",
            "102/102 [==============================] - 236s 2s/step - loss: 0.0259 - matthews_correlation_coefficient: 0.9819 - val_loss: 0.0842 - val_matthews_correlation_coefficient: 0.9519\n",
            "63/63 [==============================] - 10s 154ms/step - loss: 0.0890 - matthews_correlation_coefficient: 0.9514\n"
          ]
        }
      ],
      "source": [
        "# Train.\n",
        "# Train.\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "output_dir = path+'/Ensemble/Original_LSTM_CHAR/'\n",
        "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"Normal_trained_Char_lstm_weights.{epoch:02d}.hdf5\")\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "model=lstm(tokenizer)\n",
        "model.fit(trainingdata, traininglabels,\n",
        "        epochs=50,\n",
        "        batch_size=1024,\n",
        "        callbacks=[custom_early_stopping,modelcheckpoint],\n",
        "        validation_split=0.20,\n",
        "        shuffle=True\n",
        "        )\n",
        "\n",
        "# Evaluate model on test data.\n",
        "score, acc = model.evaluate(testingdata, testinglabels, verbose=1, batch_size=1024)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data.\n",
        "score, acc = model.evaluate(testingdata, testinglabels, verbose=1, batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nI-_F6IpDoP",
        "outputId": "a0209cfa-79d0-4622-9aef-3846e4e02f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 11s 171ms/step - loss: 0.0890 - matthews_correlation_coefficient: 0.9514\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainingdata, traininglabels,\n",
        "        epochs=50,\n",
        "        batch_size=1024,\n",
        "        callbacks=[custom_early_stopping,modelcheckpoint],\n",
        "        validation_split=0.20,\n",
        "        shuffle=True\n",
        "        )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFS4rEano-nD",
        "outputId": "59ccfcdf-8206-481a-bb95-ac3c5423af89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "102/102 [==============================] - 246s 2s/step - loss: 0.0236 - matthews_correlation_coefficient: 0.9835 - val_loss: 0.0836 - val_matthews_correlation_coefficient: 0.9538\n",
            "Epoch 2/50\n",
            "102/102 [==============================] - 259s 3s/step - loss: 0.0217 - matthews_correlation_coefficient: 0.9849 - val_loss: 0.0896 - val_matthews_correlation_coefficient: 0.9520\n",
            "Epoch 3/50\n",
            "102/102 [==============================] - 248s 2s/step - loss: 0.0222 - matthews_correlation_coefficient: 0.9847 - val_loss: 0.0855 - val_matthews_correlation_coefficient: 0.9533\n",
            "Epoch 4/50\n",
            "102/102 [==============================] - 243s 2s/step - loss: 0.0211 - matthews_correlation_coefficient: 0.9853 - val_loss: 0.0885 - val_matthews_correlation_coefficient: 0.9511\n",
            "Epoch 5/50\n",
            "102/102 [==============================] - 242s 2s/step - loss: 0.0204 - matthews_correlation_coefficient: 0.9856 - val_loss: 0.0917 - val_matthews_correlation_coefficient: 0.9526\n",
            "Epoch 6/50\n",
            "102/102 [==============================] - 247s 2s/step - loss: 0.0204 - matthews_correlation_coefficient: 0.9858 - val_loss: 0.0876 - val_matthews_correlation_coefficient: 0.9528\n",
            "Epoch 7/50\n",
            "102/102 [==============================] - 235s 2s/step - loss: 0.0182 - matthews_correlation_coefficient: 0.9880 - val_loss: 0.0948 - val_matthews_correlation_coefficient: 0.9524\n",
            "Epoch 8/50\n",
            "102/102 [==============================] - 239s 2s/step - loss: 0.0187 - matthews_correlation_coefficient: 0.9869 - val_loss: 0.0913 - val_matthews_correlation_coefficient: 0.9545\n",
            "Epoch 9/50\n",
            "102/102 [==============================] - 241s 2s/step - loss: 0.0199 - matthews_correlation_coefficient: 0.9859 - val_loss: 0.0972 - val_matthews_correlation_coefficient: 0.9532\n",
            "Epoch 10/50\n",
            "102/102 [==============================] - 239s 2s/step - loss: 0.0190 - matthews_correlation_coefficient: 0.9866 - val_loss: 0.0970 - val_matthews_correlation_coefficient: 0.9528\n",
            "Epoch 11/50\n",
            "102/102 [==============================] - 238s 2s/step - loss: 0.0183 - matthews_correlation_coefficient: 0.9879 - val_loss: 0.0923 - val_matthews_correlation_coefficient: 0.9539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdde5bb6090>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp9wh8D-cqLv"
      },
      "outputs": [],
      "source": [
        "model.save(path+'/Ensemble/Orginal_LSTM_CHAR/Finalmodel.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data.\n",
        "score, acc = model.evaluate(testingdata, testinglabels, verbose=1, batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZO0kRLyqI13",
        "outputId": "f333ce5d-9bee-4e3e-9258-ca9380ad9231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 52s 834ms/step - loss: 0.0958 - matthews_correlation_coefficient: 0.9539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca3pmUsMuf4Z"
      },
      "source": [
        "#LOAD Already saved Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgdugL2dFBX3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7quJqfQ4QIjT"
      },
      "outputs": [],
      "source": [
        "LSTM_model=tf.keras.models.load_model(path+'/Ensemble/Original_LSTM_CHAR/Normal_trained_Char_lstm_weights.05.hdf5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJVV1MizOQcM"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on test data.\n",
        "from sklearn.metrics import matthews_corrcoef,f1_score,precision_score,recall_score,roc_auc_score,auc,confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJci4pGfujgp"
      },
      "source": [
        "#Testing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaKRqZOLQIjT"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(testingdata)\n",
        "y_prediction=[]\n",
        "y_test=testinglabels\n",
        "for y in y_pred:\n",
        "    if(y>=0.5):\n",
        "        y_prediction.append(1)\n",
        "    else:\n",
        "        y_prediction.append(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjG3cIt1yf6p"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef,f1_score,precision_score,recall_score,roc_auc_score,auc,confusion_matrix,accuracy_score\n",
        "\n",
        "import csv\n",
        "MCC=matthews_corrcoef(np.array(y_test), np.array(y_prediction))\n",
        "Accuracy=accuracy_score(y_test, y_prediction)\n",
        "Fscore=f1_score(np.array(y_test), np.array(y_prediction), average='macro')\n",
        "Recall=recall_score(np.array(y_test), np.array(y_prediction), average='macro'),\n",
        "precision=precision_score(np.array(y_test), np.array(y_prediction), average='macro')\n",
        "AUC = roc_auc_score(np.array(y_test), np.array(y_prediction))\n",
        "cm= confusion_matrix(np.array(y_test), np.array(y_prediction)) \n",
        "tn= cm[0, 0]\n",
        "fp=cm[0, 1]\n",
        "fn=cm[1, 0]\n",
        "tp= cm[1, 1]\n",
        "totalpos=(fn+tp)\n",
        "totalneg=(fp+tn)\n",
        "FPR=fp/totalneg\n",
        "FNR=fn/totalpos\n",
        "Overall=open(path+'/Ensemble/validation_results_new'+'.csv','w+',encoding='utf-8',newline='')\n",
        "overall=csv.writer(Overall)\n",
        "overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "overall.writerow([MCC,Accuracy,precision,Recall,Fscore,AUC,FPR,FNR]) \n",
        "Overall.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "SDaXVEU-QIjU",
        "outputId": "c6c99d93-d436-4691-bd3e-6d1094ae5dd0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9509afeb8646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatthews_corrcoef\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mauc_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbest_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mfpr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import matthews_corrcoef,f1_score,precision_score,recall_score,roc_auc_score,auc,confusion_matrix\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prediction)\n",
        "auc_ = auc(fpr, tpr)\n",
        "best_threshold = thresholds[np.argmax(-fpr + tpr)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "lcHuygyMQIjU",
        "outputId": "c2e1c9e8-4cd0-49ff-f5f5-a65aae1705c6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"Recurrent\"\n",
        "model_full_name = \"RNN model with Character Level Embedding\"\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label=f'{model_name} (area = {auc_:.4f})')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title(f'ROC curve for {model_full_name}')\n",
        "plt.legend(loc='best')\n",
        "plt.savefig(path+'_roc.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km4CtavwjMtf",
        "outputId": "a75e9b18-2986-4072-b919-b6b5d6e74c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9619810938925422, [0.05719982087612152, 0.9809876680374146], 0.9810123769890965, (0.9809687178941703,), 0.9809856957011167, 0.9809687178941703, 0.016655836382913233, 0.021406727828746176]\n"
          ]
        }
      ],
      "source": [
        "print([MCC,acc,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0actxs0aQIjV"
      },
      "source": [
        "# Adversarial Testing Popular Seeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgUYPtm7QIjW"
      },
      "outputs": [],
      "source": [
        "def testPopularData(atype):\n",
        "    if(atype=='AllAdversary'):\n",
        "        data_dir_path = path+'/Adversaries/'\n",
        "        A1 = pd.read_csv(data_dir_path+'DomainAdversary.csv',encoding=\"utf-8\") \n",
        "        A1['Adversary']='Domain'\n",
        "        A2 = pd.read_csv(data_dir_path+'TLDAdversary.csv',encoding=\"utf-8\") \n",
        "        A2['Adversary']='TLD'\n",
        "        A3 = pd.read_csv(data_dir_path+'PathAdversary.csv',encoding=\"utf-8\") \n",
        "        A3['Adversary']='Path'\n",
        "        #A4=pd.read_csv(data_dir_path+'dmswap.csv',encoding=\"utf-8\") \n",
        "\n",
        "        A3=A3.loc[0:len(A1),:]\n",
        "        #AV4=A4\n",
        "        Mali=pd.concat([A1, A2, A3]).reset_index() \n",
        "        \n",
        "        \n",
        "         \n",
        "    else:\n",
        "        data_dir_path = path+'/Adversaries/'+atype\n",
        "        Malicious = pd.read_csv(data_dir_path+'.csv',encoding=\"utf-8\")\n",
        "        if(atype=='PathAdversary'):\n",
        "             Malicious = (Malicious)\n",
        "             Malicious=Malicious.reset_index() \n",
        "             Mali=Malicious.loc[0:50000,:]\n",
        "             \n",
        "             \n",
        "        else:\n",
        "            Mali=Malicious\n",
        "        Mali['Adversary']=atype    \n",
        "    \n",
        "    DeceptiveURLs=Mali['craftedurl']\n",
        "    BenignURLs=Mali['seedurl'].drop_duplicates()    \n",
        "    labeld=np.ones(len(DeceptiveURLs))\n",
        "    labelb=np.zeros(len(BenignURLs))\n",
        "    frame1=([DeceptiveURLs,BenignURLs])\n",
        "    url=np.concatenate(frame1).tolist()\n",
        "    label=(np.concatenate((labeld,labelb)))\n",
        "    label=pd.Series(label.tolist())\n",
        "    d={'url':url,'label':label,'adversary':Mali['Adversary'],'type':Mali['adversarytype'],'method':Mali['adversarymethod']}\n",
        "    url_data=pd.DataFrame(d, columns=['url','label','adversary','type','method'])\n",
        "    print(len(url_data))\n",
        "    return url_data['url'],url_data['label'],url_data['adversary'],url_data['type'],url_data['method']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-MA946jQIjY",
        "outputId": "76011a1b-b170-4873-809a-2f5a1a57aca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35190\n",
            "0.6398124467178176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50004\n",
            "0.0\n",
            "10049\n",
            "0.0\n",
            "12586\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,4):\n",
        "        if(j==3):\n",
        "          atype='DomainAdversary'          \n",
        "        if j==2:\n",
        "          atype='TLDAdversary'\n",
        "        if j==1:\n",
        "         atype='PathAdversary'\n",
        "        if j==0:\n",
        "          atype='AllAdversary'\n",
        "\n",
        "        adversarialURLs,adversarialLabels,adversary,adversaryType,adversaryMethod=testPopularData(atype)\n",
        "        d={'url':adversarialURLs,'label':adversarialLabels}\n",
        "        dataset_test=pd.DataFrame(d, columns=['url','label'])\n",
        "        test_sequences = tokenizer.texts_to_sequences(dataset_test['url'])\n",
        "        X_test = pad_sequences(test_sequences, maxlen=maxlen)\n",
        "        y_test = dataset_test['label']\n",
        "        y_pred = LSTM_model.predict(X_test)\n",
        "        y_prediction=[]\n",
        "        for y in y_pred:\n",
        "            if(y>=0.5):\n",
        "                y_prediction.append(1)\n",
        "            else:\n",
        "                y_prediction.append(0)\n",
        "        \n",
        "        advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "        MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "        Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "        Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "        precision=precision_score(y_test, y_prediction, average='macro')\n",
        "        AUC = roc_auc_score(y_test, y_prediction)\n",
        "        cm= confusion_matrix(y_test, y_prediction) \n",
        "        tn= cm[0, 0]\n",
        "        fp=cm[0, 1]\n",
        "        fn=cm[1, 0]\n",
        "        tp= cm[1, 1]\n",
        "        totalpos=(fn+tp)\n",
        "        totalneg=(fp+tn)\n",
        "        FPR=fp/totalneg\n",
        "        FNR=fn/totalpos\n",
        "        Overall=open(path+'/Popular/'+atype+'LSTM_targets_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        overall=csv.writer(Overall)\n",
        "        overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "        overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "        Overall.close()                \n",
        "        Details=open(path+'/Popular/'+atype+'LSTM_targets_validation_result_details'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        detail=csv.writer(Details)\n",
        "        detail.writerow(['adversarialURL','adversarialLabel','ADVERSARY','TYPE','METHOD','result'])\n",
        "        correct=0\n",
        "        if(atype=='AllAdversary'):\n",
        "            for i,y in enumerate(y_prediction):\n",
        "                \n",
        "                if(y!=y_test[i]):\n",
        "                    \n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'0'])\n",
        "                else:\n",
        "                    correct+=1\n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'1'])\n",
        "        print(correct/len(y_prediction))\n",
        "        Details.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxDPHQtVCmUt"
      },
      "outputs": [],
      "source": [
        "def testFutureData(atype):\n",
        "    if(atype=='AllAdversary'):\n",
        "        data_dir_path = path+'/Adversaries/Test_'\n",
        "        A1 = pd.read_csv(data_dir_path+'DomainAdversary.csv',encoding=\"utf-8\") \n",
        "        A1['Adversary']='Domain'\n",
        "        A2 = pd.read_csv(data_dir_path+'TLDAdversary.csv',encoding=\"utf-8\") \n",
        "        A2['Adversary']='TLD'\n",
        "        A3 = pd.read_csv(data_dir_path+'PathAdversary.csv',encoding=\"utf-8\") \n",
        "        A3['Adversary']='Path'\n",
        "        #A4=pd.read_csv(data_dir_path+'dmswap.csv',encoding=\"utf-8\") \n",
        "\n",
        "        A3=A3.loc[0:len(A1),:]\n",
        "        #AV4=A4\n",
        "        Mali=pd.concat([A1, A2, A3]).reset_index() \n",
        "        \n",
        "        \n",
        "         \n",
        "    else:\n",
        "        data_dir_path = path+'/Adversaries/'+atype\n",
        "        Malicious = pd.read_csv(data_dir_path+'.csv',encoding=\"utf-8\")\n",
        "        if(atype=='PathAdversary'):\n",
        "             Malicious = (Malicious)\n",
        "             Malicious=Malicious.reset_index() \n",
        "             Mali=Malicious.loc[0:50000,:]\n",
        "             \n",
        "             \n",
        "        else:\n",
        "            Mali=Malicious\n",
        "        Mali['Adversary']=atype    \n",
        "    \n",
        "    DeceptiveURLs=Mali['craftedurl']\n",
        "    BenignURLs=Mali['seedurl'].drop_duplicates()    \n",
        "    labeld=np.ones(len(DeceptiveURLs))\n",
        "    labelb=np.zeros(len(BenignURLs))\n",
        "    frame1=([DeceptiveURLs,BenignURLs])\n",
        "    url=np.concatenate(frame1).tolist()\n",
        "    label=(np.concatenate((labeld,labelb)))\n",
        "    label=pd.Series(label.tolist())\n",
        "    d={'url':url,'label':label,'adversary':Mali['Adversary'],'type':Mali['adversarytype'],'method':Mali['adversarymethod']}\n",
        "    url_data=pd.DataFrame(d, columns=['url','label','adversary','type','method'])\n",
        "    print(len(url_data))\n",
        "    return url_data['url'],url_data['label'],url_data['adversary'],url_data['type'],url_data['method']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUbj81g7QIjZ"
      },
      "source": [
        "# FuturePrediction_Adversarial_Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VsA2dIUQIjZ",
        "outputId": "42bf22f1-8fbd-49a9-8100-626fdaf5c5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "84405\n",
            "0.053681653930454355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50004\n",
            "0.0\n",
            "10049\n",
            "0.0\n",
            "12586\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,4):\n",
        "        if(j==3):\n",
        "          atype='DomainAdversary'          \n",
        "        if j==2:\n",
        "          atype='TLDAdversary'\n",
        "        if j==1:\n",
        "         atype='PathAdversary'\n",
        "        if j==0:\n",
        "          atype='AllAdversary'\n",
        "\n",
        "        adversarialURLs,adversarialLabels,adversary,adversaryType,adversaryMethod=testFutureData(atype)\n",
        "        d={'url':adversarialURLs,'label':adversarialLabels}\n",
        "        dataset_test=pd.DataFrame(d, columns=['url','label'])\n",
        "        test_sequences = tokenizer.texts_to_sequences(dataset_test['url'])\n",
        "        X_test = pad_sequences(test_sequences, maxlen=maxlen)\n",
        "        y_test = dataset_test['label']\n",
        "        y_pred = LSTM_model.predict(X_test)\n",
        "        y_prediction=[]\n",
        "        for y in y_pred:\n",
        "            if(y>=0.5):\n",
        "                y_prediction.append(1)\n",
        "            else:\n",
        "                y_prediction.append(0)\n",
        "        \n",
        "        advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "        MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "        Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "        Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "        precision=precision_score(y_test, y_prediction, average='macro')\n",
        "        AUC = roc_auc_score(y_test, y_prediction)\n",
        "        cm= confusion_matrix(y_test, y_prediction) \n",
        "        tn= cm[0, 0]\n",
        "        fp=cm[0, 1]\n",
        "        fn=cm[1, 0]\n",
        "        tp= cm[1, 1]\n",
        "        totalpos=(fn+tp)\n",
        "        totalneg=(fp+tn)\n",
        "        FPR=fp/totalneg\n",
        "        FNR=fn/totalpos\n",
        "        Overall=open(path+'/Future/'+atype+'LSTM_targets_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        overall=csv.writer(Overall)\n",
        "        overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "        overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "        Overall.close()                \n",
        "        Details=open(path+'/Future/'+atype+'LSTM_targets_validation_result_details'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        detail=csv.writer(Details)\n",
        "        detail.writerow(['adversarialURL','adversarialLabel','ADVERSARY','TYPE','METHOD','result'])\n",
        "        correct=0\n",
        "        if(atype=='AllAdversary'):\n",
        "            for i,y in enumerate(y_prediction):\n",
        "                \n",
        "                if(y!=y_test[i]):\n",
        "                    \n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'0'])\n",
        "                else:\n",
        "                    correct+=1\n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'1'])\n",
        "        print(correct/len(y_prediction))\n",
        "        Details.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPjJ1Qii4Z4c"
      },
      "source": [
        "#VISUAL SIMILARITY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIPAObx45UMn",
        "outputId": "cf780097-8605-4c25-db63-44c26dfaf025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: missingno in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from missingno) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from missingno) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from missingno) (1.21.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from missingno) (0.11.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (1.4.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->missingno) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->missingno) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->missingno) (1.15.0)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->missingno) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->missingno) (2022.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.3.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.23.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from tldextract) (2.10)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract) (3.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from requests-file>=1.4->tldextract) (1.15.0)\n",
            "Installing collected packages: requests-file, tldextract\n",
            "Successfully installed requests-file-1.5.1 tldextract-3.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting homoglyphs\n",
            "  Downloading homoglyphs-2.0.4-py3-none-any.whl (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 7.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: homoglyphs\n",
            "Successfully installed homoglyphs-2.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting validators\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators) (4.4.2)\n",
            "Building wheels for collected packages: validators\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19582 sha256=2b553e63cd10c2699cfc62924866bea6043f2f47e47724e36b458562b2e1640c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/55/ab/36a76989f7f88d9ca7b1f68da6d94252bb6a8d6ad4f18e04e9\n",
            "Successfully built validators\n",
            "Installing collected packages: validators\n",
            "Successfully installed validators-0.20.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rfc3986\n",
            "  Downloading rfc3986-2.0.0-py2.py3-none-any.whl (31 kB)\n",
            "Installing collected packages: rfc3986\n",
            "Successfully installed rfc3986-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install missingno\n",
        "!pip install seaborn\n",
        "!pip install tldextract\n",
        "!pip install homoglyphs\n",
        "!pip install nltk\n",
        "!pip install validators\n",
        "!pip install rfc3986"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-We2gSp_u2G",
        "outputId": "d275b247-9410-4afb-f33e-9f004257a4ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading twitter - 1grams ...\n",
            "Reading twitter - 2grams ...\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "#Creating URL embeddings\n",
        "from ekphrasis.classes.segmenter import Segmenter\n",
        "import re\n",
        "seg_eng = Segmenter(corpus=\"twitter\",max_split_length=40) \n",
        "\n",
        "import pickle ,os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import time\n",
        "from collections import Counter\n",
        "from urllib.parse import urlparse\n",
        "import tldextract\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import collections\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sys, email\n",
        "import pandas as pd \n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import collections\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
        "import lightgbm as lgb\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Aug  1 11:42:35 2019\n",
        "\n",
        "@author: bushra\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import ensemble\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import csv\n",
        "from numpy import average\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn import preprocessing\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import coverage_error\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import average_precision_score\n",
        "# Load CSV (using python)\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "from sklearn.metrics import label_ranking_average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def lgb_f1_score(y_hat, data):\n",
        "    y_true = data.get_label()\n",
        "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
        "    return 'f1', f1_score(y_true, y_hat), True\n",
        "\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import sort\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import itertools,re\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# Load datasets\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn import feature_extraction\n",
        "\n",
        " # -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Thu Jul 25 14:28:29 2019\n",
        "\n",
        "@author: bushra\n",
        "\"\"\"\n",
        "import random\n",
        "import validators\n",
        "from urllib.parse import urlunparse,urlparse\n",
        "import re\n",
        "import pandas as pd\n",
        "#from rfc3986 import urlparse\n",
        "from rfc3986 import is_valid_uri\n",
        "import itertools\n",
        "import tldextract\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import homoglyphs as hg,os\n",
        "import sys \n",
        "import numpy as np\n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "from difflib import SequenceMatcher\n",
        "import homoglyphs as hg,os\n",
        "from urllib.request import Request, urlopen\n",
        "import time\n",
        "start = time.time()\n",
        "homoglyphs =hg.Homoglyphs(languages={'en'},\n",
        "            strategy=hg.STRATEGY_LOAD,\n",
        "            ascii_strategy=hg.STRATEGY_REMOVE\n",
        "        )\n",
        "import random\n",
        "import validators\n",
        "import re\n",
        "import pandas as pd\n",
        "from rfc3986 import urlparse\n",
        "from rfc3986 import is_valid_uri\n",
        "import itertools\n",
        "import sys \n",
        "from nltk import FreqDist\n",
        "from nltk.corpus import gutenberg\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import re\n",
        "import random\n",
        "import csv\n",
        "from nltk.corpus import brown\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "import spacy\n",
        "import pandas as pd \n",
        "import codecs,re\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "codecs.register_error(\"strict\", codecs.ignore_errors)\n",
        "import re\n",
        "#path=\"E:\\\\PrimaryStudy1\\\\Code\\\\part2\\\\wordembeddingDataset\\\\glove.840B.300d\\\\\"\n",
        "#word2vec_output_file = path+'glove.840B.300d.txt.word2vec'\n",
        "##glove2word2vec(path+'glove.840B.300d.txt', word2vec_output_file)\n",
        "#model = KeyedVectors.load_word2vec_format(word2vec_output_file)\n",
        "#print(model.wv.most_similar(positive='secure'))\n",
        "#wordsindex2word=model.wv.index2word\n",
        "      \n",
        "#words = set(brown.words())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql5ezFhB4cy0"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def findIP(List):\n",
        "        M=List[0]\n",
        "        ip = re.findall( r'[0-9-]+(?:\\.[0-9]+){3}', M )\n",
        "        if ip !=[]:\n",
        "            List.remove(M)\n",
        "        else:\n",
        "            ip=''\n",
        "        return str(ip)\n",
        "\n",
        "def findProtocol(List):\n",
        "   \n",
        "    for M in List:\n",
        "        if(':' in M):\n",
        "            protocol=M.split(':').pop(0)\n",
        "            List.remove(M)\n",
        "            return protocol\n",
        "        else:\n",
        "            return ''\n",
        "        \n",
        "def findDomains(List):\n",
        "     domain=''\n",
        "     sld=''\n",
        "     tld=''\n",
        "     port=''\n",
        "     for M in List:\n",
        "         if('.' in M):\n",
        "             splitlist=M.split('.')\n",
        "             if(M.count('.')>1):\n",
        "                 tld=splitlist.pop()\n",
        "                 ld=tld.split(':')\n",
        "                 tld=ld.pop(0)\n",
        "                 if(ld!=[]):\n",
        "                     port=ld.pop()\n",
        "                 sld=splitlist.pop()\n",
        "                 for i,l in enumerate(splitlist):\n",
        "                    domain+=l\n",
        "                    if(i!=len(splitlist)-1):\n",
        "                        domain+='.'\n",
        "                 List.remove(M)\n",
        "                 return tld,sld,domain,port\n",
        "             else:\n",
        "                tld=splitlist.pop()\n",
        "                ld=tld.split(':')\n",
        "                tld=ld.pop(0)\n",
        "                if(ld!=[]):\n",
        "                     port=ld.pop()\n",
        "                domain=splitlist.pop()\n",
        "                List.remove(M)\n",
        "                return tld,sld,domain,port\n",
        "     return tld,sld,domain,port\n",
        "def get_words(path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    \n",
        "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.read().splitlines()\n",
        "        return stopwords    \n",
        "def urltokenizer(url):\n",
        "    tld='None'\n",
        "    sld='None'\n",
        "    domain=''\n",
        "    port='None'\n",
        "    path='None'\n",
        "    exe='None'\n",
        "    protocol=''\n",
        "    urlparsed = urlparse(url)\n",
        "    Major=url.split('/')  #[http:,www.google.com.sg, webhp?hl=zh-CN] \n",
        "    ip= (findIP(Major))\n",
        "    tem=urlparsed.scheme\n",
        "    if(tem is not None):\n",
        "        if('.' in tem):\n",
        "            Lst=tem.split('/')\n",
        "            [tld,sld,domain,port]=(findDomains(Lst))\n",
        "            \n",
        "        else:\n",
        "            protocol=urlparsed.scheme\n",
        "    \n",
        "    userinfo=urlparsed.userinfo\n",
        "    host=urlparsed.host\n",
        "    pathurl=urlparsed.path\n",
        "    if(pathurl is not None and host is None):\n",
        "        Lst=pathurl.split('/')\n",
        "        if('.' in pathurl):\n",
        "            [tld,sld,domain,port]=(findDomains(Lst))\n",
        "            [path,exe]=(findPathQueryFragment(Lst))\n",
        "        else:\n",
        "            path=pathurl\n",
        "    elif(host is not None and pathurl is None):\n",
        "        Lst=host.split('/')\n",
        "        ip= (findIP(Lst))\n",
        "        if('.' in host):\n",
        "            [tld,sld,domain,port]=(findDomains(Lst))\n",
        "            [path,exe]=(findPathQueryFragment(Lst))\n",
        "        else:\n",
        "            path+=host\n",
        "    elif(host is not None and pathurl is not None):\n",
        "        Lst=host.split('/')\n",
        "        ip= (findIP(Lst))\n",
        "        if('.' in host):\n",
        "            [tld,sld,domain,port]=(findDomains(Lst))\n",
        "        if('.' in pathurl):\n",
        "            Lst1=pathurl.split('/')\n",
        "            if(Lst!=[]):\n",
        "                for q in Lst:\n",
        "                    Lst1.append(Lst.remove(q))\n",
        "            [path,exe]=(findPathQueryFragment(Lst1))\n",
        "        if('.' not in host):\n",
        "            path+=host\n",
        "        if('.' not in pathurl):\n",
        "            path+=pathurl\n",
        "        \n",
        "            \n",
        "    \n",
        "    parameter=urlparsed.query\n",
        "    fragment=urlparsed.fragment\n",
        "    [sld,domain,tld]=tldextract.extract(url)\n",
        "    if(path=='/'):\n",
        "        path=''\n",
        "    if(domain!=''):\n",
        "        domain=' '.join(converter(tokenparts(domain.split('.'))))\n",
        "    if(domain=='' and ip!=''):\n",
        "        domain=ip\n",
        "    if(path!=None and path!='' and path!='/'):\n",
        "    \n",
        "        path=' '.join(converter(tokenparts(re.findall(r\"[\\w']+\", path))))\n",
        "    else:\n",
        "        path='None'\n",
        "    if(sld!='None' and sld!='' and sld!=None):\n",
        "    \n",
        "        sld=' '.join(converter(tokenparts(sld.split('.'))))\n",
        "    else:\n",
        "        sld='www'\n",
        "    if(parameter!=None):\n",
        "        parameter=' '.join(converter(tokenparts(re.findall(r\"[\\w']+\", parameter))))\n",
        "    else:\n",
        "        parameter='None'\n",
        "    if(fragment!=None):\n",
        "        fragment=' '.join(converter(tokenparts(re.findall(r\"[\\w']+\", fragment))))\n",
        "    else:\n",
        "        fragment='None'\n",
        "    if(exe==''):\n",
        "        exe='None'\n",
        "    return [ip,port,protocol,tld,(sld),userinfo,host,domain,(path),(parameter),(fragment),exe]\n",
        "          \n",
        "       \n",
        "                \n",
        "def findPathQueryFragment(List):\n",
        "    path=''\n",
        "    query=''\n",
        "    fragment=''\n",
        "    exe=''\n",
        "    remaining=''\n",
        "    last='p'\n",
        "    previousch=''\n",
        "   #eas?camp=1932-1;cre=mu&grpid=1738&tag_id=618&nums=FGApbjFAAA\n",
        "\n",
        "    for i,M in enumerate(List):\n",
        "        remaining=''\n",
        "        if('.' in M and i==len(List)-1):  \n",
        "            newList=M.split('.') \n",
        "            exe+=newList.pop()\n",
        "            for s,q in enumerate(newList):\n",
        "                if(q is not None):\n",
        "                    path+='/'+newList.pop(s)\n",
        "           \n",
        "                  \n",
        "        else:\n",
        "            if(i<len(List)-1):\n",
        "                path+=M+'/'\n",
        "            else:\n",
        "                path+=M\n",
        "        \n",
        "                \n",
        "      \n",
        "           \n",
        "         \n",
        "    return path,exe   \n",
        "def convert(ls,op): \n",
        "      \n",
        "    # Converting integer list to string list \n",
        "    s = [str(i) for i in ls] \n",
        "      \n",
        "    # Join list items using join() \n",
        "    res = (\"op\".join(s)) \n",
        "      \n",
        "    return(res) \n",
        "\n",
        "# Extract NLP features\n",
        "\n",
        "# config 1: Bag-of-word without tf-idf\n",
        "# config 2: Bag-of-word with tf-idf\n",
        "# config 3: N-gram without tf-idf\n",
        "# config 4: N-gram with tf-idf\n",
        "_digits = re.compile('\\d')\n",
        "def contains_digits(d):\n",
        "     return bool(_digits.search(d))\n",
        "def tokenparts(words):\n",
        "    finalwords=[]\n",
        "    for w in words:\n",
        "       w=re.sub('\\W+',' SPCHAR ',w)\n",
        "       w=re.sub('\\d+',' DIGITCHAR ',w)\n",
        "       w=re.sub('_',' ',w)\n",
        "       w=re.sub(';',' ',w)\n",
        "       \n",
        "       if(w!='' and w!='None' ):\n",
        "#           words.append(w.split('_'))\n",
        "#           words.remove(w)\n",
        "           try: \n",
        "              subwords=(seg_eng.segment(w.strip()))         \n",
        "              finalwords.append(subwords.split(' '))\n",
        "           except:\n",
        "             finalwords.append(w.strip())\n",
        "    return (finalwords)\n",
        "\n",
        "\n",
        "def converter(listoflist):\n",
        "    lst=[]\n",
        "   \n",
        "    for ls in listoflist:\n",
        "        \n",
        "        if(type(ls) == list):\n",
        "            for e in ls:\n",
        "                if(' ' in e):\n",
        "                  \n",
        "                  for ele in (e.split(' ')):\n",
        "                    \n",
        "                    lst.append(ele)\n",
        "                elif(e !=''):\n",
        "                    lst.append(e)\n",
        "        else:\n",
        "           lst.append(ls)\n",
        "    \n",
        "    return lst\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn0t-ts1C5UZ"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def getword_tokens(url):\n",
        "           \n",
        "                 wordlist=''.join(url)\n",
        "                 [ip,port,protocol,tld,(sld),userinfo,host,domain,(path),(parameter),(fragment),exe] =urltokenizer(wordlist)\n",
        "                 if(ip is not None and ip !='None' and ip.strip()!=''):\n",
        "                   domain='ipaddress'\n",
        "                 if(port is not None and port !='None' and port.strip()!=''):\n",
        "                   domain='ipaddress:port' \n",
        "                 wordtoken=[port,protocol,userinfo,(sld),domain,tld,(path),(parameter),(fragment),exe] \n",
        "                 finalwordtokens=[ ]\n",
        "                 for v in wordtoken:\n",
        "                   if(v is not None and v !='' and v is not 'None'):\n",
        "                     \n",
        "                     if(type(v) == list):\n",
        "                         finalwordtokens.append(v)\n",
        "                     else:\n",
        "                         v=v.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "                         if(' ' in v):\n",
        "                           v=v.split(' ')\n",
        "                         if(type(v)==list):\n",
        "                           finalwordtokens.append(v)\n",
        "                         else:\n",
        "                          finalwordtokens.append([v])\n",
        "                     \n",
        "                 \n",
        "                 wordtokens= list(itertools.chain.from_iterable(finalwordtokens))\n",
        "                 if('' in wordtokens):\n",
        "                   wordtokens.remove('')\n",
        "                 \n",
        "                 return  wordtokens\n",
        "              \n",
        "                 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N17VUs7F_94P"
      },
      "outputs": [],
      "source": [
        "oaurls,oalabels=LegtrainingData()\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "model_dbow=Doc2Vec.load(path+'Models/Visualsimilarity.d2v')\n",
        "!pip install textdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqU54BQyAKua",
        "outputId": "4adcec63-68ef-4ddf-8769-6a43d0d2c6f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: textdistance in /usr/local/lib/python3.7/dist-packages (4.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install textdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ9tj3CU_co3",
        "outputId": "8ff5fdb4-3964-4215-ee1b-e0c4d5839295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "http://www.frostjedi.com/vex/\n"
          ]
        }
      ],
      "source": [
        "import editdistance\n",
        "import textdistance\n",
        "url='www.netflix.com/'\n",
        "wordtoken=getword_tokens(url)\n",
        "model_dbow.random.seed(0)\n",
        "new_vector = model_dbow.infer_vector(wordtoken,steps=20, alpha=0.025)\n",
        "sim=model_dbow.docvecs.most_similar([new_vector])\n",
        "labelssim=[]\n",
        "levdistance=[]\n",
        "lcsdistance=[]\n",
        "jarodistance=[]\n",
        "similarity_weight=[]\n",
        "for s in sim:\n",
        "        simurl=oaurls[s[0]]\n",
        "        print(simurl)\n",
        "        simwordtoken=' '.join(getword_tokens(simurl))\n",
        "        labelssim.append(oalabels[s[0]])\n",
        "        similarity_weight.append(s[1])\n",
        "        levdistance.append(textdistance.jaccard.similarity(simwordtoken, ' '.join(wordtoken)))\n",
        "        jarodistance.append(textdistance.jaro.similarity(simwordtoken, ' '.join(wordtoken)))\n",
        "        lcsdistance.append(textdistance.lcsstr.similarity(simwordtoken, ' '.join(wordtoken)))\n",
        "proz=labelssim.count(0)/10\n",
        "pro1=labelssim.count(1)/10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpNskp-b9AIY"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import utils\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import multiprocessing\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AN6NzMI9B1Y"
      },
      "outputs": [],
      "source": [
        "X,y=LegtrainingData()\n",
        "X_train, L_X_test, y_train, L_y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEpBTLdoAvEO"
      },
      "outputs": [],
      "source": [
        "X_train=list(X_train)\n",
        "y_train=list(y_train)\n",
        "L_X_test=list(L_X_test)\n",
        "L_y_test=list(L_y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUELh1I4ndzm"
      },
      "outputs": [],
      "source": [
        "def testData(L_X_test):\n",
        "\n",
        "    data_dir_path = path\n",
        "    Legitimate = L_X_test\n",
        "    Malicious = pd.read_csv(data_dir_path+'Phishing//Phish_Training.csv',encoding=\"utf-8\") \n",
        "    mal=len(Malicious)\n",
        "    Leg=Legitimate[:mal]\n",
        "    frame1=([Leg,Malicious['url']])\n",
        "    url=np.concatenate(frame1).tolist()\n",
        "    leg=len(Leg)\n",
        "    \n",
        "    print(leg)\n",
        "    print(mal)\n",
        "    label0=np.zeros((leg))\n",
        "    label1=np.ones((mal))\n",
        "    label=(np.concatenate((label0,label1)))\n",
        "    label=pd.Series(label.tolist())\n",
        "    d={'url':url,'label':label}\n",
        "    url_data=pd.DataFrame(d, columns=['url','label'])\n",
        "    return url_data['url'],url_data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F9kDDyln5W5",
        "outputId": "02b23aae-fa5a-47f6-a980-10bf379dd007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96693\n",
            "96693\n"
          ]
        }
      ],
      "source": [
        "X,y=testData(L_X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NqT1kNLMZ69",
        "outputId": "cdca59b8-d7ac-447c-9af4-a164874972ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "193386"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLOjjMQjiDlm"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTpZJaKHTQZJ",
        "outputId": "6689c448-228d-491d-96e8-abac6f941c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "135370"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7ptEh8gv_XF"
      },
      "outputs": [],
      "source": [
        "X_train=list(X_train)\n",
        "y_train=list(y_train)\n",
        "X_test=list(X_test)\n",
        "y_test=list(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI5enqinMun1"
      },
      "outputs": [],
      "source": [
        "def main(urllist,labels):\n",
        "    #urls,labels=trainingData()\n",
        "    wordtokens=[]\n",
        "    urlwords=[]\n",
        "    for i,url in enumerate(urllist):\n",
        "              try:\n",
        "                 \n",
        "                 words=getword_tokens(url)\n",
        "                 urlwords.append(words)\n",
        "                 wordtokens.append(TaggedDocument(words,tags=[int(labels[i])]))\n",
        "              except Exception as e:\n",
        "                print(\"ERRRORRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\")\n",
        "                print(i, url)\n",
        "                break\n",
        "            \n",
        "    \n",
        "    with open('urlwords.pkl', 'wb') as f:\n",
        "        pickle.dump(urlwords, f)   \n",
        "    with open('urlwordstagged.pkl', 'wb') as f:\n",
        "        pickle.dump(wordtokens, f) \n",
        "    return (wordtokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "km9apzPeTPCy",
        "outputId": "71b145fa-4a02-4a2d-9fc9-75cee010c5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TaggedDocument(['i', 'back', 'eb', 'digitchar', 'a', 'com', 'scam', 'a', 'paypal', 'i', 'digitchar'], [1])\n"
          ]
        }
      ],
      "source": [
        "test_documents=main(X_test,y_test)\n",
        "print(test_documents[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi8il5tTCrTT"
      },
      "outputs": [],
      "source": [
        "print(X_train[0])\n",
        "print(getword_tokens(X_train[0]))\n",
        "main(X_train[0],y_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXN72U6M-b2u",
        "outputId": "8607e986-baed-4527-9da2-2201927f7d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TaggedDocument(['http', 'www', 'ipaddress', 'images', 'asinhas', 'digitchar', 'html'], [1])\n"
          ]
        }
      ],
      "source": [
        "train_documents=main(X_train,y_train)\n",
        "print(train_documents[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwgEo-23MUUb"
      },
      "outputs": [],
      "source": [
        "cores = multiprocessing.cpu_count()\n",
        "model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPbhq-o0SyFp"
      },
      "outputs": [],
      "source": [
        "model_dbow=Doc2Vec.load(path+'Models/Visualsimilarity.d2v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "otOgPgg1-zit",
        "outputId": "aa53bfb4-7e00-44a7-9105-d61bfa05ae6b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c4995d26e9f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_dbow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'Models/Visualsimilarity.d2v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install textdistance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
          ]
        }
      ],
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "model_dbow=Doc2Vec.load(path+'Models/Visualsimilarity.d2v')\n",
        "!pip install textdistance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ot86H2zBFGim",
        "outputId": "ea891584-d139-4364-db8b-f40aa0f5c7c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 734001/734001 [00:00<00:00, 3063282.41it/s]\n"
          ]
        }
      ],
      "source": [
        "model_dbow.build_vocab([x for x in tqdm(train_documents)])\n",
        "train_documents  = utils.shuffle(train_documents)\n",
        "model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n",
        "\n",
        "model_dbow.save(path+'Visualsimilarity.d2v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9nkheGhUScF"
      },
      "outputs": [],
      "source": [
        "def vector_for_learning(model, input_docs):\n",
        "    sents = input_docs\n",
        "    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
        "    return targets, feature_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cy7GsWpZMjaS"
      },
      "outputs": [],
      "source": [
        "y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
        "y_test, X_test = vector_for_learning(model_dbow, test_documents)\n",
        "\n",
        "# logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "# logreg.fit(X_train, y_train)\n",
        "# y_pred = logreg.predict(X_test)\n",
        "# print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))\n",
        "# print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JFJK26IK2Uf",
        "outputId": "987d7189-419d-41de-e5f2-ea661bcfdd40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['MCC', 'accuracy', 'precision', 'recall', 'f1', 'auc', 'FPR', 'FNR']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import matthews_corrcoef,f1_score,precision_score,recall_score,roc_auc_score,auc,confusion_matrix,accuracy_score\n",
        "# RFT_DOC =RandomForestClassifier(n_jobs=1, random_state=42)\n",
        "# RFT_DOC.fit(X_train, y_train)\n",
        "RFT_DOC=pickle.load(open(path+'/Models/visual_similarity_rft.pkl', \"rb\"))\n",
        "y_pred = RFT_DOC.predict(X_test)\n",
        "y_prediction=y_pred\n",
        "advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "precision=precision_score(y_test, y_prediction, average='macro')\n",
        "AUC = roc_auc_score(y_test, y_prediction)\n",
        "cm= confusion_matrix(y_test, y_prediction) \n",
        "tn= cm[0, 0]\n",
        "fp=cm[0, 1]\n",
        "fn=cm[1, 0]\n",
        "tp= cm[1, 1]\n",
        "totalpos=(fn+tp)\n",
        "totalneg=(fp+tn)\n",
        "FPR=fp/totalneg\n",
        "FNR=fn/totalpos\n",
        "Overall=open(path+'/Models/'+'Visual_Similarity_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "overall=csv.writer(Overall)\n",
        "overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "print([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos])\n",
        "overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "Overall.close()        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNAlIkNwYuw6",
        "outputId": "29e22e30-9849-4027-ccc6-68b8be43203f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.813746652728812,\n",
              " 0.9068532818532818,\n",
              " 0.9069113781701348,\n",
              " (0.9068352781170439,),\n",
              " 0.9068464180839546,\n",
              " 0.906835278117044,\n",
              " 0.09890375903447798,\n",
              " 0.08742568473143407]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEZjIfZ3OZY-"
      },
      "outputs": [],
      "source": [
        "def testPopularData(atype):\n",
        "    if(atype=='AllAdversary'):\n",
        "        data_dir_path = path+'/Adversaries/'\n",
        "        A1 = pd.read_csv(data_dir_path+'DomainAdversary.csv',encoding=\"utf-8\") \n",
        "        A1['Adversary']='Domain'\n",
        "        A2 = pd.read_csv(data_dir_path+'TLDAdversary.csv',encoding=\"utf-8\") \n",
        "        A2['Adversary']='TLD'\n",
        "        A3 = pd.read_csv(data_dir_path+'PathAdversary.csv',encoding=\"utf-8\") \n",
        "        A3['Adversary']='Path'\n",
        "        #A4=pd.read_csv(data_dir_path+'dmswap.csv',encoding=\"utf-8\") \n",
        "\n",
        "        A3=A3.loc[0:len(A1),:]\n",
        "        #AV4=A4\n",
        "        Mali=pd.concat([A1, A2, A3]).reset_index() \n",
        "        \n",
        "        \n",
        "         \n",
        "    else:\n",
        "        data_dir_path = path+'/Adversaries/'+atype\n",
        "        Malicious = pd.read_csv(data_dir_path+'.csv',encoding=\"utf-8\")\n",
        "        if(atype=='PathAdversary'):\n",
        "             Malicious = (Malicious)\n",
        "             Malicious=Malicious.reset_index() \n",
        "             Mali=Malicious.loc[0:50000,:]\n",
        "             \n",
        "             \n",
        "        else:\n",
        "            Mali=Malicious\n",
        "        Mali['Adversary']=atype    \n",
        "    \n",
        "    DeceptiveURLs=Mali['craftedurl']\n",
        "    BenignURLs=Mali['seedurl'].drop_duplicates()    \n",
        "    labeld=np.ones(len(DeceptiveURLs))\n",
        "    labelb=np.zeros(len(BenignURLs))\n",
        "    frame1=([DeceptiveURLs,BenignURLs])\n",
        "    url=np.concatenate(frame1).tolist()\n",
        "    label=(np.concatenate((labeld,labelb)))\n",
        "    label=pd.Series(label.tolist())\n",
        "    d={'url':url,'label':label,'adversary':Mali['Adversary'],'type':Mali['adversarytype'],'method':Mali['adversarymethod']}\n",
        "    url_data=pd.DataFrame(d, columns=['url','label','adversary','type','method'])\n",
        "    print(len(url_data))\n",
        "    return url_data['url'],url_data['label'],url_data['adversary'],url_data['type'],url_data['method']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lT0OP4HO3wc",
        "outputId": "c05945c4-dd75-4d8b-881d-6511b20c81ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35190\n",
            "0.4887752202330207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50004\n",
            "0.0\n",
            "10049\n",
            "0.0\n",
            "12586\n",
            "0.0\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,4):\n",
        "        if(j==3):\n",
        "          atype='DomainAdversary'          \n",
        "        if j==2:\n",
        "          atype='TLDAdversary'\n",
        "        if j==1:\n",
        "         atype='PathAdversary'\n",
        "        if j==0:\n",
        "          atype='AllAdversary'\n",
        "\n",
        "        adversarialURLs,adversarialLabels,adversary,adversaryType,adversaryMethod=testPopularData(atype)\n",
        "        test_documents_popular=main(adversarialURLs,adversarialLabels)\n",
        "        y_test, X_test = vector_for_learning(model_dbow, test_documents_popular)\n",
        "        y_pred = RFT_DOC.predict(X_test)\n",
        "        y_prediction=[]\n",
        "        for y in y_pred:\n",
        "            if(y>=0.5):\n",
        "                y_prediction.append(1)\n",
        "            else:\n",
        "                y_prediction.append(0)\n",
        "        \n",
        "        advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "        MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "        Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "        Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "        precision=precision_score(y_test, y_prediction, average='macro')\n",
        "        AUC = roc_auc_score(y_test, y_prediction)\n",
        "        cm= confusion_matrix(y_test, y_prediction) \n",
        "        tn= cm[0, 0]\n",
        "        fp=cm[0, 1]\n",
        "        fn=cm[1, 0]\n",
        "        tp= cm[1, 1]\n",
        "        totalpos=(fn+tp)\n",
        "        totalneg=(fp+tn)\n",
        "        FPR=fp/totalneg\n",
        "        FNR=fn/totalpos\n",
        "        Overall=open(path+'/Popular/'+atype+'_targets_validation_results_DOC2VEC'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        overall=csv.writer(Overall)\n",
        "        overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "        overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "        Overall.close()                \n",
        "        Details=open(path+'/Popular/'+atype+'_targets_validation_result_details_DOC2VEC'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        detail=csv.writer(Details)\n",
        "        detail.writerow(['adversarialURL','adversarialLabel','ADVERSARY','TYPE','METHOD','result'])\n",
        "        correct=0\n",
        "        if(atype=='AllAdversary'):\n",
        "            for i,y in enumerate(y_prediction):\n",
        "                \n",
        "                if(y!=y_test[i]):\n",
        "                    \n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'0'])\n",
        "                else:\n",
        "                    correct+=1\n",
        "                    detail.writerow([adversarialURLs[i],adversarialLabels[i],adversary[i],adversaryType[i],adversaryMethod[i],'1'])\n",
        "        print(correct/len(y_prediction))\n",
        "        Details.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwUzv5voQCeH",
        "outputId": "f9663057-9df4-4826-eea9-9aeeff71544d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.019937036329654174, 0.38193230573653264, 0.5007539315914229, (0.6318042054915689,), 0.27791003292356964, 0.6318042054915689, 0.11764705882352941, 0.6187445301933328]\n"
          ]
        }
      ],
      "source": [
        "print([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4h5DPmzvbGh"
      },
      "source": [
        "#Load Basic Lexical + External Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVWirwSVYc1i",
        "outputId": "ca4a7f3c-3798-4f06-fe67-e1137936faae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xgboost==1.4.2\n",
            "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 19 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost==1.4.2) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost==1.4.2) (1.21.6)\n",
            "Installing collected packages: xgboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost==1.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjgwpZF6SWJK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "import numpy as np\n",
        "ord_enc = OrdinalEncoder()\n",
        "def testPopularData_BLE(atype):\n",
        "       filename=path+'/Ensemble/BLE/test'+atype+'Features.csv'\n",
        "       Featureset = pd.read_csv(filename,encoding=\"utf-8\") \n",
        "       Featureset=Featureset.fillna(-1)\n",
        "       Featureset=Featureset.dropna()\n",
        "       Featureset[\"country\"] = ord_enc.fit_transform(Featureset[[\"country\"]])\n",
        "       labels=Featureset['label']\n",
        "       urls=Featureset['url']\n",
        "       print(np.shape(Featureset))\n",
        "       Features=Featureset.drop(['url','label'], axis=1)\n",
        "       print(np.shape(Features))\n",
        "       Features=Features.replace('?',-1)\n",
        "       Features=Features.replace('TRUE',1)\n",
        "       Features=Features.replace('True',1)\n",
        "       Features=Features.replace('true',1)\n",
        "       Features=Features.replace('FALSE',0)\n",
        "       Features=Features.replace('False',0)\n",
        "       Features=Features.replace('false',0)\n",
        "       Features=Features.replace('0',0)\n",
        "       return Features,urls,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "co7jvvE0InEJ",
        "outputId": "eae621a8-797a-4805-af25-89ce5b2cec27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(193337, 152)\n",
            "(193337, 150)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import make_scorer,balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support, accuracy_score\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import matthews_corrcoef,f1_score,precision_score,recall_score,roc_auc_score,auc,confusion_matrix\n",
        "xgb=XGBClassifier(max_depth=51,tree_method='auto')\n",
        "Train_Features,X,y=train_BLE()\n",
        "# enumerate splits\n",
        "def FPR(clf, X, y):\n",
        "     y_pred = clf.predict(X)\n",
        "     cm= confusion_matrix(y, y_pred) \n",
        "     tn= cm[0, 0]\n",
        "     fp=cm[0, 1]\n",
        "     fn=cm[1, 0]\n",
        "     tp= cm[1, 1]\n",
        "     totalpos=(fn+tp)\n",
        "     totalneg=(fp+tn)\n",
        "             \n",
        "     return fp/totalneg\n",
        "custom_scorer = {'accuracy': make_scorer(accuracy_score),\n",
        "                 'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
        "                 'precision': make_scorer(precision_score, average='macro'),\n",
        "                 'recall': make_scorer(recall_score, average='macro'),\n",
        "                 'f1': make_scorer(f1_score, average='macro'),\n",
        "                 'auc':'roc_auc',\n",
        "                 'Matthew': make_scorer(matthews_corrcoef),\n",
        "                 'FPR':FPR\n",
        "                 }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V35RcMakN2OR",
        "outputId": "6f83a4c9-fc95-4616-df98-d0305d8caffd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 0, -1,  1])"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Train_Features['spf'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIPugj0SLoZF",
        "outputId": "0f2ee57a-8a25-4e97-a31e-aa29cf11224a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.9749333520421926, 0.9749250811904602, 0.9749245198547284, 0.9969751366843784, 0.026992684324780004, 0.025074918809539826]\n"
          ]
        }
      ],
      "source": [
        "kf = StratifiedKFold(n_splits=10,random_state=42,shuffle=True)\n",
        "scores = cross_validate(xgb, Train_Features, y,cv=kf,scoring=custom_scorer,n_jobs=-1)\n",
        "xgb.fit(Train_Features,y)\n",
        "result=[scores['test_precision'].mean(),scores['test_recall'].mean(),scores['test_f1'].mean(),scores['test_auc'].mean(),scores['test_FPR'].mean(),(1-scores['test_recall'].mean())]\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ngT2VnFBLBsU",
        "outputId": "e7e25d78-1f1f-4406-c449-07ac734718a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-11289d03-e246-4310-86e2-f6a9bf24f87e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qtd_ponto_url</th>\n",
              "      <th>qtd_hifen_url</th>\n",
              "      <th>qtd_underline_url</th>\n",
              "      <th>qtd_barra_url</th>\n",
              "      <th>qtd_interrogacao_url</th>\n",
              "      <th>qtd_igual_url</th>\n",
              "      <th>qtd_arroba_url</th>\n",
              "      <th>qtd_comercial_url</th>\n",
              "      <th>qtd_exclamacao_url</th>\n",
              "      <th>qtd_espaco_url</th>\n",
              "      <th>...</th>\n",
              "      <th>S</th>\n",
              "      <th>T</th>\n",
              "      <th>U</th>\n",
              "      <th>V</th>\n",
              "      <th>W</th>\n",
              "      <th>X</th>\n",
              "      <th>Y</th>\n",
              "      <th>Z</th>\n",
              "      <th>shortnighservice</th>\n",
              "      <th>entropy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.902175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>16</td>\n",
              "      <td>7</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>5.619995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.599358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.221097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5.208210</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193332</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.780395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193333</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4.064355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193334</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>4.621557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193335</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.725481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193336</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3.463281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>193337 rows × 150 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11289d03-e246-4310-86e2-f6a9bf24f87e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-11289d03-e246-4310-86e2-f6a9bf24f87e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-11289d03-e246-4310-86e2-f6a9bf24f87e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        qtd_ponto_url  qtd_hifen_url  qtd_underline_url  qtd_barra_url  \\\n",
              "0                   2              0                  0              0   \n",
              "1                   3              0                  0              6   \n",
              "2                   4              0                  0              2   \n",
              "3                   2              0                  0              2   \n",
              "4                   4              0                  1              3   \n",
              "...               ...            ...                ...            ...   \n",
              "193332              4              0                  0              0   \n",
              "193333              2              0                  0              1   \n",
              "193334              2              0                  0              3   \n",
              "193335              2              0                  0              0   \n",
              "193336              2              0                  0              1   \n",
              "\n",
              "        qtd_interrogacao_url  qtd_igual_url  qtd_arroba_url  \\\n",
              "0                          0              0               0   \n",
              "1                          0              0               0   \n",
              "2                          0              0               0   \n",
              "3                          0              0               0   \n",
              "4                          0              1               0   \n",
              "...                      ...            ...             ...   \n",
              "193332                     0              0               0   \n",
              "193333                     0              0               0   \n",
              "193334                     0              0               0   \n",
              "193335                     0              0               0   \n",
              "193336                     0              0               0   \n",
              "\n",
              "        qtd_comercial_url  qtd_exclamacao_url  qtd_espaco_url  ...   S   T  U  \\\n",
              "0                       0                   0               0  ...   0   2  0   \n",
              "1                       0                   0               0  ...  17  16  7   \n",
              "2                       0                   0               0  ...   0   1  0   \n",
              "3                       0                   0               0  ...   3   2  0   \n",
              "4                       0                   0               0  ...   6   2  2   \n",
              "...                   ...                 ...             ...  ...  ..  .. ..   \n",
              "193332                  0                   0               0  ...   1   3  2   \n",
              "193333                  0                   0               0  ...   1   3  0   \n",
              "193334                  0                   0               0  ...   1  11  1   \n",
              "193335                  0                   0               0  ...   2   2  1   \n",
              "193336                  0                   0               0  ...   2   2  0   \n",
              "\n",
              "         V   W  X  Y  Z  shortnighservice   entropy  \n",
              "0        0   3  1  0  0                 1  3.902175  \n",
              "1       11  11  2  8  5                 1  5.619995  \n",
              "2        0   1  0  0  0                 1  3.599358  \n",
              "3        0   3  0  0  0                 1  3.221097  \n",
              "4        5   6  1  1  0                 1  5.208210  \n",
              "...     ..  .. .. .. ..               ...       ...  \n",
              "193332   1   4  0  0  0                 1  3.780395  \n",
              "193333   1   3  0  2  1                 1  4.064355  \n",
              "193334   1   0  1  1  0                 1  4.621557  \n",
              "193335   0   0  0  0  0                 1  3.725481  \n",
              "193336   0   3  0  0  0                 1  3.463281  \n",
              "\n",
              "[193337 rows x 150 columns]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Train_Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qy-jan7Y-Gk0",
        "outputId": "51fa38f3-981a-4aad-8ed4-bc931f2eb3e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.045232149373655765, 0.6033688225011918, 0.5016969454981034, (0.8014161826716525,), 0.3794868490658859, 0.8014161826716525, 0.0, 0.397167634656695]\n"
          ]
        }
      ],
      "source": [
        "print([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLVarjTMo7cn"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "file_name = path+\"/Ensemble/BLE/xgbmodels.pkl\"\n",
        "\n",
        "# save\n",
        "pickle.dump(xgb, open(file_name, \"wb\"))\n",
        "\n",
        "# load\n",
        "xgb_model_loaded = pickle.load(open(file_name, \"rb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_WnruPpU5K9",
        "outputId": "db472868-e63f-462c-dca1-a0ed9fe12323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(35190, 152)\n",
            "(35190, 150)\n",
            "(50021, 152)\n",
            "(50021, 150)\n",
            "(10049, 152)\n",
            "(10049, 150)\n",
            "(12586, 152)\n",
            "(12586, 150)\n"
          ]
        }
      ],
      "source": [
        "for j in range(0,4):\n",
        "        if(j==3):\n",
        "          atype='DomainAdversary'          \n",
        "        if j==2:\n",
        "          atype='TLDAdversary'\n",
        "        if j==1:\n",
        "         atype='PathAdversary'\n",
        "        if j==0:\n",
        "          atype='AllAdversary'\n",
        "\n",
        "        Features,urls,labels=testPopularData_BLE(atype)\n",
        "        X_test = Features\n",
        "        y_test = labels\n",
        "        y_pred = xgb_model_loaded.predict(X_test)\n",
        "        \n",
        "        \n",
        "        y_prediction=[]\n",
        "        for y in y_pred:\n",
        "            if(y>=0.5):\n",
        "                y_prediction.append(1)\n",
        "            else:\n",
        "                y_prediction.append(0)\n",
        "        \n",
        "        advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "        MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "        Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "        Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "        precision=precision_score(y_test, y_prediction, average='macro')\n",
        "        AUC = roc_auc_score(y_test, y_prediction)\n",
        "        cm= confusion_matrix(y_test, y_prediction) \n",
        "        tn= cm[0, 0]\n",
        "        fp=cm[0, 1]\n",
        "        fn=cm[1, 0]\n",
        "        tp= cm[1, 1]\n",
        "        totalpos=(fn+tp)\n",
        "        totalneg=(fp+tn)\n",
        "        FPR=fp/totalneg\n",
        "        FNR=fn/totalpos\n",
        "        Overall=open(path+'/Popular/'+atype+'BLE_targets_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        overall=csv.writer(Overall)\n",
        "        overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "        overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "        Overall.close()                \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5SUN3c-Uu4h"
      },
      "outputs": [],
      "source": [
        "def testFutureData_BLE(atype):\n",
        "       filename=path+'/Ensemble/BLE/test'+atype+'FutureFeatures.csv'\n",
        "       Featureset = pd.read_csv(filename,encoding=\"utf-8\") \n",
        "       Featureset=Featureset.fillna(-1)\n",
        "       Featureset=Featureset.dropna()\n",
        "       Featureset[\"country\"] = ord_enc.fit_transform(Featureset[[\"country\"]])\n",
        "       future_labels=Featureset['label']\n",
        "       future_urls=Featureset['url']\n",
        "       Features=Featureset.drop(['url','label'], axis=1)\n",
        "       Features=Features.replace('?',-1)\n",
        "       Features=Features.replace('0',0)\n",
        "       Features=Features.replace('TRUE',1)\n",
        "       Features=Features.replace('True',1)\n",
        "       Features=Features.replace('true',1)\n",
        "       Features=Features.replace('FALSE',0)\n",
        "       Features=Features.replace('False',0)\n",
        "       Features=Features.replace('false',0)\n",
        "       return Features,future_urls, future_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZTLw9PiU4fA"
      },
      "outputs": [],
      "source": [
        "for j in range(0,4):\n",
        "        if(j==3):\n",
        "          atype='DomainAdversary'          \n",
        "        if j==2:\n",
        "          atype='TLDAdversary'\n",
        "        if j==1:\n",
        "         atype='PathAdversary'\n",
        "        if j==0:\n",
        "          atype='AllAdversary'\n",
        "\n",
        "        Future_Features,future_urls, future_labels=testPopularData_BLE(atype)\n",
        "        X_test = Future_Features\n",
        "        y_test = future_labels\n",
        "        y_pred = xgb_model_loaded.predict(X_test)\n",
        "        y_prediction=[]\n",
        "        for y in y_pred:\n",
        "            if(y>=0.5):\n",
        "                y_prediction.append(1)\n",
        "            else:\n",
        "                y_prediction.append(0)\n",
        "        \n",
        "        advAccuracy=accuracy_score(y_test, y_prediction)\n",
        "        MCC=matthews_corrcoef(y_test, y_prediction)\n",
        "        Fscore=f1_score(y_test, y_prediction, average='macro')\n",
        "        Recall=recall_score(y_test, y_prediction, average='macro'),\n",
        "        precision=precision_score(y_test, y_prediction, average='macro')\n",
        "        AUC = roc_auc_score(y_test, y_prediction)\n",
        "        cm= confusion_matrix(y_test, y_prediction) \n",
        "        tn= cm[0, 0]\n",
        "        fp=cm[0, 1]\n",
        "        fn=cm[1, 0]\n",
        "        tp= cm[1, 1]\n",
        "        totalpos=(fn+tp)\n",
        "        totalneg=(fp+tn)\n",
        "        FPR=fp/totalneg\n",
        "        FNR=fn/totalpos\n",
        "        Overall=open(path+'/Popular/'+atype+'xgb_BLE_targets_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "        overall=csv.writer(Overall)\n",
        "        overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "        overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "        Overall.close()                \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U69S0nzB_db_"
      },
      "outputs": [],
      "source": [
        "Ensemble_labels=[]\n",
        "finalProbabilities=[]\n",
        "Features,urls,labels=testPopularData_BLE('AllAdversary')\n",
        "x_test_BLP=Features\n",
        "test_sequences = tokenizer.texts_to_sequences(urls)\n",
        "x_test_LSTM = pad_sequences(test_sequences, maxlen=maxlen)\n",
        "prediction_prob_BLE=BLE_lgbm.predict_proba(x_test_BLP)\n",
        "prediction_prob=model.predict(x_test_LSTM)\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXHy8OFHguyi"
      },
      "outputs": [],
      "source": [
        "prediction_prob_XGB=BLE_xgb.predict_proba(x_test_BLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hKHz6ueTU9w"
      },
      "outputs": [],
      "source": [
        "Ensemble_labels=[]\n",
        "for i in range(0,len(urls)):\n",
        "  prediction_LSTM=[0,0]\n",
        "  if(prediction_prob[i]>=0.5):\n",
        "    prediction_LSTM[1]=float(prediction_prob[i])\n",
        "    prediction_LSTM[0]=float(1-prediction_prob[i])\n",
        "  else:\n",
        "    prediction_LSTM[0]=float(prediction_prob[i])\n",
        "    prediction_LSTM[1]=float(1-prediction_prob[i])\n",
        "  print(urls[i])\n",
        "   \n",
        "  Probabilities=(prediction_prob_BLE[i]+np.array(prediction_LSTM))/2\n",
        "  print(Probabilities)\n",
        "  finalProbabilities.append(Probabilities)\n",
        "  Ensemble_labels.append(np.argmax(Probabilities))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yfol5_-XSIRt",
        "outputId": "e7b7162e-727a-4286-f152-7a5c6651a847"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9151438105831877"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(np.array(labels),Ensemble_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Op-7xG4J-wQ3"
      },
      "outputs": [],
      "source": [
        "advAccuracy=accuracy_score(labels, Ensemble_labels)\n",
        "MCC=matthews_corrcoef(labels, Ensemble_labels)\n",
        "Fscore=f1_score(labels, Ensemble_labels, average='macro')\n",
        "Recall=recall_score(labels, Ensemble_labels, average='macro')\n",
        "precision=precision_score(labels, Ensemble_labels, average='macro')\n",
        "AUC = roc_auc_score(labels, Ensemble_labels)\n",
        "cm= confusion_matrix(labels, Ensemble_labels) \n",
        "tn= cm[0, 0]\n",
        "fp=cm[0, 1]\n",
        "fn=cm[1, 0]\n",
        "tp= cm[1, 1]\n",
        "totalpos=(fn+tp)\n",
        "totalneg=(fp+tn)\n",
        "FPR=fp/totalneg\n",
        "FNR=fn/totalpos\n",
        "Overall=open(path+'/Popular/'+atype+'Ensemble_targets_validation_results'+'.csv','w+',encoding='utf-8',newline='')\n",
        "overall=csv.writer(Overall)\n",
        "overall.writerow(['MCC','accuracy','precision','recall','f1','auc','FPR','FNR']) \n",
        "overall.writerow([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos]) \n",
        "Overall.close()                \n",
        "Details=open(path+'/Popular/'+atype+'Ensemble_targets_validation_result_details'+'.csv','w+',encoding='utf-8',newline='')\n",
        "detail=csv.writer(Details)\n",
        "detail.writerow(['adversarialURL','adversarialLabel','ADVERSARY','TYPE','METHOD','result'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHjMO9Rc-ncx",
        "outputId": "40e62103-df8d-4a6b-84c7-60ef1be65509"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.05863429717325823, 0.9151438105831877, 0.5038622289594599, 0.7225386455003674, 0.486116737308255, 0.7225386455003674, 0.47058823529411764, 0.08433447370514759]\n"
          ]
        }
      ],
      "source": [
        "print([MCC,advAccuracy,precision,Recall,Fscore,AUC,fp/totalneg,fn/totalpos])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_r3H9AuXVaK"
      },
      "source": [
        "#ENSEMBLE PREDICTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwavE10UXXYM"
      },
      "outputs": [],
      "source": [
        "# example of a super learner model for regression\n",
        "from math import sqrt\n",
        "from numpy import hstack\n",
        "from numpy import vstack\n",
        "from numpy import asarray\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# create a list of base-models\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append([XGBClassifier(max_depth=51,tree_method='auto'),'xgb'])\n",
        "\tmodels.append([RandomForestClassifier(criterion='entropy',max_depth=79,n_estimators=57,n_jobs=-1,random_state=42),'rf'])\n",
        "\tmodels.append([LSTM_model,'lstm'])\n",
        "\tmodels.append([RandomForestClassifier(n_jobs=1, random_state=42),'rfdoc'])\n",
        "\treturn models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "-NpGkVIDir5t",
        "outputId": "2c95322f-824f-438b-aeed-85f42a1a16cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train (500, 100) (500,) Test (500, 100) (500,)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-44d67f76fc01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;31m# get models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;31m# get out of fold predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mmeta_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_out_of_fold_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3103ced1b288>\u001b[0m in \u001b[0;36mget_models\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtree_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'xgb'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m79\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLSTM_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lstm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rfdoc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LSTM_model' is not defined"
          ]
        }
      ],
      "source": [
        "# collect out of fold predictions form k-fold cross validation\n",
        "# def get_out_of_fold_predictions(models):\n",
        "#   meta_X, meta_y = list(), list()\n",
        "#   # define split of data\n",
        "#   kfold = KFold(n_splits=10, shuffle=True)\n",
        "#   Train_Features,X,y=train_BLE()\n",
        "# # enumerate splits\n",
        "#   for train_ix, test_ix in kfold.split(X):\n",
        "#     fold_yhats = list()\n",
        "#     # get data\n",
        "#     train_X, test_X = X[train_ix], X[test_ix]\n",
        "#     train_X_features,test_X_features=Train_Features[train_ix],Train_Features[test_ix]\n",
        "#     train_y, test_y = y[train_ix], y[test_ix]\n",
        "#     meta_y.extend(test_y)\n",
        "    \n",
        "# \t\t# fit and make predictions with each sub-model\n",
        "#     for modellist in models:\n",
        "#       model,name=modellist\n",
        "#       if(name=='xgb' or name=='rf'):\n",
        "#           model.fit(train_X_features, train_y)\n",
        "#           yhat = model.predict(test_X_features)\n",
        "#       elif(name=='lstm'):\n",
        "#           samples=train_X\n",
        "#           max_chars = 20000\n",
        "#           maxlen = 128\n",
        "#           tokenizer = Tokenizer(num_words=max_chars, char_level=True)\n",
        "#           tokenizer.fit_on_texts(samples)\n",
        "#           sequences = tokenizer.texts_to_sequences(samples)\n",
        "#           word_index = tokenizer.word_index\n",
        "#           print('Found %s unique tokens.' % len(word_index))\n",
        "#           data = pad_sequences(sequences, maxlen=maxlen)\n",
        "#           model=lstm(tokenizer)\n",
        "#           model.fit(data, train_y,\n",
        "#                     epochs=50,\n",
        "#                     batch_size=1200,\n",
        "#                     callbacks=[custom_early_stopping,modelcheckpoint],\n",
        "#                     validation_split=0.20,\n",
        "#                     shuffle=True\n",
        "#                     )\n",
        "#           test_sequences = tokenizer.texts_to_sequences(test_X)\n",
        "#           test_data=pad_sequences(test_sequences, maxlen=maxlen)\n",
        "#           y_hat=model.predict(test_data)\n",
        "\n",
        "# \t\t\t# store columns\n",
        "#     fold_yhats.append(yhat.reshape(len(yhat),1))\n",
        "#     # store fold yhats as columns\n",
        "#     meta_X.append(hstack(fold_yhats))\n",
        "#   return vstack(meta_X), asarray(meta_y)\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "# def fit_base_models(X, y, models):\n",
        "# \tfor model in models:\n",
        "# \t\tmodel.fit(X, y)\n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "\tmodel = LinearRegression()\n",
        "\tmodel.fit(X, y)\n",
        "\treturn model\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(X, y, models):\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict(X)\n",
        "\t\tmse = mean_squared_error(y, yhat)\n",
        "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "\tmeta_X = list()\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict(X)\n",
        "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
        "\tmeta_X = hstack(meta_X)\n",
        "\t# predict\n",
        "\treturn meta_model.predict(meta_X)\n",
        "\n",
        "# create the inputs and outputs\n",
        "X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
        "# split\n",
        "X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
        "print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
        "# get models\n",
        "models = get_models()\n",
        "# get out of fold predictions\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)\n",
        "# fit base models\n",
        "fit_base_models(X, y, models)\n",
        "# fit the meta model\n",
        "meta_model = fit_meta_model(meta_X, meta_y)\n",
        "# evaluate base models\n",
        "evaluate_models(X_val, y_val, models)\n",
        "# evaluate meta model\n",
        "yhat = super_learner_predictions(X_val, models, meta_model)\n",
        "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAAzaq5uH_iB"
      },
      "outputs": [],
      "source": [
        "# collect out of fold predictions form k-fold cross validation\n",
        "def get_out_of_fold_predictions(models):\n",
        "  meta_X, meta_y = list(), list()\n",
        "  # define split of data\n",
        "  kfold = KFold(n_splits=10, shuffle=True)\n",
        "  Train_Features,X,y=train_BLE()\n",
        "# enumerate splits\n",
        "  for train_ix, test_ix in kfold.split(X):\n",
        "    fold_yhats = list()\n",
        "    # get data\n",
        "    train_X, test_X = X[train_ix], X[test_ix]\n",
        "    train_X_features,test_X_features=Train_Features[train_ix],Train_Features[test_ix]\n",
        "    train_y, test_y = y[train_ix], y[test_ix]\n",
        "    meta_y.extend(test_y)\n",
        "    \n",
        "\t\t# fit and make predictions with each sub-model\n",
        "    for modellist in models:\n",
        "      model,name=modellist\n",
        "      if(name=='xgb' or name=='rf'):\n",
        "          model.fit(train_X_features, train_y)\n",
        "          yhat = model.predict(test_X_features)\n",
        "      elif(name=='lstm'):\n",
        "          samples=train_X\n",
        "          max_chars = 20000\n",
        "          maxlen = 128\n",
        "          tokenizer = Tokenizer(num_words=max_chars, char_level=True)\n",
        "          tokenizer.fit_on_texts(samples)\n",
        "          sequences = tokenizer.texts_to_sequences(samples)\n",
        "          word_index = tokenizer.word_index\n",
        "          print('Found %s unique tokens.' % len(word_index))\n",
        "          data = pad_sequences(sequences, maxlen=maxlen)\n",
        "          model=lstm(tokenizer)\n",
        "          model.fit(data, train_y,\n",
        "                    epochs=50,\n",
        "                    batch_size=1200,\n",
        "                    callbacks=[custom_early_stopping,modelcheckpoint],\n",
        "                    validation_split=0.20,\n",
        "                    shuffle=True\n",
        "                    )\n",
        "          test_sequences = tokenizer.texts_to_sequences(test_X)\n",
        "          test_data=pad_sequences(test_sequences, maxlen=maxlen)\n",
        "          y_hat=model.predict(test_data)\n",
        "      else:\n",
        "          train_documents=main(train_X,train_y)\n",
        "          test_documents=main(test_X,test_y)\n",
        "          y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
        "          y_test, X_test = vector_for_learning(model_dbow, test_documents)\n",
        "          model.fit(X_train,y_train)\n",
        "          y_hat=model.predict(X_test)\n",
        "\n",
        "\n",
        "\t\t\t# store columns\n",
        "    fold_yhats.append(yhat.reshape(len(yhat),1))\n",
        "    # store fold yhats as columns\n",
        "    meta_X.append(hstack(fold_yhats))\n",
        "  return vstack(meta_X), asarray(meta_y)\n",
        "\n",
        "# fit all base models on the training dataset\n",
        "def fit_base_models(models):\n",
        "\t# fit and make predictions with each sub-model\n",
        "    Train_Features,train_X,train_y=train_BLE()\n",
        "    for modellist in models:\n",
        "      model,name=modellist\n",
        "      if(name=='xgb' or name=='rf'):\n",
        "          model.fit(Train_Features, train_y)\n",
        "          \n",
        "      elif(name=='lstm'):\n",
        "          samples=train_X\n",
        "          max_chars = 20000\n",
        "          maxlen = 128\n",
        "          tokenizer = Tokenizer(num_words=max_chars, char_level=True)\n",
        "          tokenizer.fit_on_texts(samples)\n",
        "          sequences = tokenizer.texts_to_sequences(samples)\n",
        "          word_index = tokenizer.word_index\n",
        "          print('Found %s unique tokens.' % len(word_index))\n",
        "          data = pad_sequences(sequences, maxlen=maxlen)\n",
        "          model=lstm(tokenizer)\n",
        "          model.fit(data, train_y,\n",
        "                    epochs=50,\n",
        "                    batch_size=1200,\n",
        "                    callbacks=[custom_early_stopping,modelcheckpoint],\n",
        "                    validation_split=0.20,\n",
        "                    shuffle=True\n",
        "                    )\n",
        "          \n",
        "      else:\n",
        "          train_documents=main(train_X,train_y)\n",
        "          y_train, X_train = vector_for_learning(model_dbow, train_documents)\n",
        "          model.fit(X_train,y_train)\n",
        "          \n",
        "\n",
        "# fit a meta model\n",
        "def fit_meta_model(X, y):\n",
        "  logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "  logreg.fit(X, y)\n",
        "  return logreg\n",
        "\n",
        "# evaluate a list of models on a dataset\n",
        "def evaluate_models(models):\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict(X)\n",
        "\t\tmse = mean_squared_error(y, yhat)\n",
        "\t\tprint('%s: RMSE %.3f' % (model.__class__.__name__, sqrt(mse)))\n",
        "\n",
        "# make predictions with stacked model\n",
        "def super_learner_predictions(X, models, meta_model):\n",
        "\tmeta_X = list()\n",
        "\tfor model in models:\n",
        "\t\tyhat = model.predict(X)\n",
        "\t\tmeta_X.append(yhat.reshape(len(yhat),1))\n",
        "\tmeta_X = hstack(meta_X)\n",
        "\t# predict\n",
        "\treturn meta_model.predict(meta_X)\n",
        "\n",
        "# # create the inputs and outputs\n",
        "# X, y = make_regression(n_samples=1000, n_features=100, noise=0.5)\n",
        "# # split\n",
        "# X, X_val, y, y_val = train_test_split(X, y, test_size=0.50)\n",
        "# print('Train', X.shape, y.shape, 'Test', X_val.shape, y_val.shape)\n",
        "# get models\n",
        "models = get_models()\n",
        "# get out of fold predictions\n",
        "meta_X, meta_y = get_out_of_fold_predictions(X, y, models)\n",
        "print('Meta ', meta_X.shape, meta_y.shape)\n",
        "# fit base models\n",
        "fit_base_models(models)\n",
        "# fit the meta model\n",
        "meta_model = fit_meta_model(meta_X, meta_y)\n",
        "# evaluate base models\n",
        "evaluate_models(models)\n",
        "# evaluate meta model\n",
        "yhat = super_learner_predictions(models, meta_model)\n",
        "print('Super Learner: RMSE %.3f' % (sqrt(mean_squared_error(y_val, yhat))))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Trained_LSTM_WORD_CHAR_EMBEDDING.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}